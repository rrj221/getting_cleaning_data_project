available.packages()
available.packages(head(rownames(a) 3))
available.packages(head(rownames(a), 3))
head(rownames(a), 3)
find.packages("devtools")
find.packages("devtools")
find.package("devtools")
install.packages("devtools")
find.package("devtools")
library(devtools)
find_rtools()
install.packages("KernSmooth")
library(KernSmooth)
getwd()
y <- data.frame(a = 1, b = "a")
dput(y)
dput(y, file = "y.R")
new.y <- dget("y.R")
new.y
setwd("C:/Users/rrj22/OneDrive/Desktop/data_science/technical_track/r_working_directory")
setwd("C:/Users/rrj22/OneDrive/Desktop/data_science/technical_track/r_working_directory")
source(swirl)
library(swirl)
swirl()
ls()
class(plants)
dim(plants)
nrow(plants)
ncol(plants)
object.size(plants)
names(plants)
head(plants)
head(plants, 10)
tail(plants, 15)
summary(plants)
table(plants$Active_Growth_Period)
str(plants)
?sample
sample(1:6, 4, replace=TRUE)
sample(1:6, 4, replace=TRUE)
sample(1:20, 4)
sample(1:20, 10)
LETTERS
sample(LETTERS)
flips <- sample(c(0,1), prob=c(.3, .7))
flips <- sample(c(0,1), 100, prob=c(.3, .7))
flips <- sample(c(0,1), 100, replace=TRUE, prob=c(.3, .7))
flips
sum(flips)
?rbinom
rbinom(1, size = 100, prob = .7)
flips2 <- rbinom(100, size = 1, prob = .7)
flips2
sum(flips2)
?rnorm
rnorm(10)
rnorm(10, mean=10, sd=25)
rnorm(10, mean=100, sd=25)
?rpois
rpois(10, mean=10)
rpois(10, 10)
rpois(5, 10)
replicate(100, rpois(5,10))
my_pois <- replicate(100, rpois(5, 10))
my_pois
cm <- colMeans(my_pois)
hist(cm)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl)
library(XML)
library(XML)
doc <- xmlTreeParse(fileUrl)
doc <- xmlTreeParse(fileUrl, useInternal=TRUE)
doc <- xmlTreeParse(fileUrl, useInternalNodes = =TRUE)
doc <- xmlTreeParse(fileUrl, useInternalNodes = TRUE)
LIBRARY(xml2)
library(xml2)
doc <- read_xml(fileUrl)
rootNode <- xml_root(doc)
xmlName(rootNode)
xml_name(rootNode)
xml_ns(rootNode)
xml_structure(rootNode)
xml_children(rootNode)
sapply(xml_children(rootNode), xml_name))
sapply(xml_children(rootNode), xml_name)
rootNode[[1]]
xml_children(rootNode)[1]
install.packages("XML")
fileUrl <- "https://www.w3schools.com/xml/simple.xml"
library(XML)
xmlTreeParse(fileUrl)
fileUrl
xmlTreeParse(fileUrl, useInternalNodes = TRUE)
file2 <- "http://www.w3schools.com/xml/simple.xml"
xmlTreeParse(file2)
rm(file2)
xmlTreeParse(fileUrl)
library(RCurl)
install.packages(RCurl)
install.packages("RCurl")
install.packages("RCurl")
library(RCurl)
xData <- getURL(fileUrl)
doc <- xmlParse(xData)
library(XML)
doc <- xmlParse(xData)
doc
fileUrl <- "https://www.w3schools.com/xml/simple.xml"
library(XML)
library(RCurl)
fileUrl <- "https://www.w3schools.com/xml/simple.xml"
xData <- getURL(fileUrl)
doc <- xmlParse(xData)
doc
xmlName(doc)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNode)
rootNode[[1]]
rootNode[[1]][[1]]
rootNode[1,1]
xmlSApply(rootNode, xmlValue)
xmlSApply(rootNode, "//name", xmlValue)
xpathSApply(rootNode, "//name", xmlValue)
xpathSApply(rootNode, "//price", xmlValue)
ravensUrl <- "http://www.espn.com/nfl/team/_/name/bal/baltimore-ravens"
docRavens <- htmlTreeParse(ravensUrl)
xmlpathSapply(docRavens, "//li[@class='score]", xmlValue)
library(XML)
xmlpathSapply(docRavens, "//li[@class='score]", xmlValue)
xpathSapply(docRavens, "//li[@class='score]", xmlValue)
xpathSApply(docRavens, "//li[@class='score]", xmlValue)
docRavens
xpathSApply(docRavens, "//div[@class='score]", xmlValue)
class(docRavens)
xpathSApply(docRavens, "//div[@class='score]", htmlValue)
xpathSApply(docRavens, "//div[@class='score]", xmlValue)
class(doc)
class(rootNode)
xpathSApply(xmlRoot(docRavens), "//div[@class='score]", xmlValue)
xmlRoot(docRavens)
ravensRoot <- xmlRoot(docRavens)
xpathSApply(ravensRoot, "//div[@class='score]", xmlValue)
?htmlTreeParse
docRavens <- htmlTreeParse(ravensUrl, useInternalNodes = TRUE)
ravensRoot <- xmlRoot(docRavens)
xpathSApply(ravensRoot, "//div[@class='score]", xmlValue)
xpathSApply(ravensRoot, "//li[@class='score]", xmlValue)
xpathSApply(ravensRoot, "//div[@class='score']", xmlValue)
xpathSApply(ravensRoot, "//li[@class='score']", xmlValue)
xpathSApply(docRavens, "//li[@class='score']", xmlValue)
scores <- xpathSApply(docRavens, "//li[@class='score']", xmlValue)
scores
list(scores)
scores <- xpathSApply(docRavens, "//div[@class='score']", xmlValue)
scores
scores <- xpathSApply(docRavens, "//div[@class="score"]", xmlValue)
scores <- xpathSApply(docRavens, "//div[@class='score']", xmlValue)
scores <- xpathSApply(docRavens, "//div[@class='logo']", xmlValue)
scores
scores <- xpathSApply(docRavens, "//div[@class='game-info']", xmlValue)
scores
docRavens <- htmlTreeParse(ravensUrl, useInternalNodes = TRUE)
scores <- xpathSApply(docRavens, "//div[@class='game-info']", xmlValue)
scores
scores <- xpathSApply(docRavens, "//div[@class='score']", xmlValue)
scores <- xpathSApply(docRavens, "//div[@class='team-name']", xmlValue)
scores <- xpathSApply(docRavens, "//div[@class='game-info']", xmlValue)
scores <- xpathSApply(docRavens, "//div[@class='game-result']", xmlValue)
scores <- xpathSApply(docRavens, "//div[@class='game-result win']", xmlValue)
write.csv(docRavens, "C/Users/rrj22/OneDrive/Desktop/ravens.csv")
saveXML(docRavens, "C/Users/rrj22/OneDrive/Desktop/ravens.csv")
saveXML(docRavens, "C/Users/rrj22/OneDrive/Desktop/ravens.xml")
saveXML(docRavens, "C\Users\rrj22\OneDrive\Desktop\ravens.xml")
saveXML(docRavens, "C:\Users\rrj22\OneDrive\Desktop\ravens.xml")
saveXML(docRavens, "*C:\Users\rrj22\OneDrive\Deskto/ravens.xml)
saveXML(docRavens, "*C:\Users\rrj22\OneDrive\Deskto/ravens.xml")
saveXML(docRavens, "*C:\Users\rrj22\OneDrive\Deskto/ravens.xml")
saveXML(docRavens, "ravens.xml")
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/rrj221/repos")
names(jsonData)
names(jsonData@owner)
names(jsonData@full_name)
names(jsonData$full_name)
names(jsonData$owner)
jsonData$owner$login
myjson <- toJSON(iris, pretty = TRUE)
cat(myjson)
myjson <- toJSON(iris, pretty = TRUE)
cat(myjson[1])
cat(myjson[1][1])
cat(myjson[[1]])
myjson[1]
clear()
myjson[1]
cat(myjson[[1]])
iris2 <- fromJSON(myjson)
iris2
iris2 <- fromJSON(myjson)
head(iris)
set.seed(123)
DT <- data.table(x=sample(letters[1:3], 1E5, TRUE))
library(data.table)
install.packages("data.table")
library(data.table)
DT <- data.table(x=sample(letters[1:3], 1E5, TRUE))
DT[, .N, by=x]
set.seed(123)
DT <- data.table(x=sample(letters[1:3], 1E5, TRUE))
DT[, .N, by=x]
DT <- data.table(x=rep("a", "b", "c"), each=100), y=rnorm(300))
DT <- data.table(x=rep(c("a", "b", "c"), each=100), y=rnorm(300))
setkey(DT, x)
DT['a']
head(DT['a'])
DT <- data.table(x=rep(c("a", "b", "c"), each=100), y=rnorm(300))
setkey(DT, x)
head(DT['a'], 10)
DT1 <- data.table(x=c('a', 'a', 'b', 'dt1'), y=1:4)
DT2 <- data.table(x=c('a', 'b', 'dt2'), y=5:7)
setkey(DT1, x); setkey(DT2, x)
merge(DT1, DT2)
library(swirl)
rm(list=ls())
swirl()
swirl()
install_from_swirl("Getting and Cleaning Data")
swirl()
swirl()
install.packages("yam1")
install.packages("yaml")
install.packages("yaml")
swirl()
library(swirl)
swirl()
library(devtools)
library(devtools)
install_github('viking/r-yaml')
swirl()
library(swirl)
swirl()
swirl()
install.packages("dplyr")
install.packages("dplyr")
swirl()
library(swirl)
swirl()
install.packages("yaml")
install.packages("yaml")
library(swirl)
swirl()
install_from_swirl("Getting And Cleaning Data")
install_from_swirl("Getting and Cleaning Data")
library(swirl)
swirl()
swirl()
swirl()
library(swirl)
install_from_swirl("Getting and Cleaning Data")
swirl()
library(dplyr)
swirl()
remove.packages("yaml")
install.packages("devtools")
library(devtools)
install_version("yaml", version="2.1.16")
swirl()
remove.packages("swirl")
remove.packages("swirl")
install.packages("swirl")
install.packages("swirl")
swirl()
library(swirl)
library(swirl)
install.packages("swirl")
library(swirl)
install.packages("swirl")
install.packages("swirl")
install.packages("swirl")
install.packages("rjava")
yes
install.packages("rJava")
install.packages("xlsx")
library(xlsx)
install.packages("RMySQL", type = "source")
library(RMySQL)
uscsDb <- dbConnect(MySQL(), user="genome", host="genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(uscsDb,"show databases;"); dbDisconnect(uscsDb)
result
hg19 <- dbConnect(MySQL(), user="genome", db="hg19", host="genome-mysql.cse.ucsc.edu")
allTables <- dbListTables(hg19)
alTables
allTables
head(allTables)
dbListFields(hg19, "affyU133Plus2")
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
dbGetQuery(hg19, "select bin from affyU133Plus2")
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
affyData <- dbReadTable(hg19, "affyU133Plus2")
head(affyData)
query <- dbSendQuery(hg19, "select * from affyU133Plus2" where misMatches between 1 and 3")
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query); quantile(affyMis$misMatches)
affyMisSmall <- fetch(query, n=10); dbClearResult(query)
dim(affyMisSmall)
affyMisSmall
dbDisconnect(hg19)
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")
N
library(rhdf5)
created = h5createFile("example.h5")
created
created = h5createGroup("example.h5", "foo")
created = h5createGroup("example.h5", "baa")
created = h5createGroup("example.h5", "foo/foobaa")
h5ls("example.h5")
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAJ&hl=en")
htmlCode = readLines(con)
close(con)
htmlCode
head(htmlCode)
url<- "http://scholar.google.com/citations?user=HI-I6C0AAAJ&hl=en"
library(XML)
html <- htmlTreeParse(url, useInternalNodes = TRUE)
library(httr)
html2 = GET("cnn.com")
content2 = content(html2, as="text")
parsedHmtl = htmlParse(content2, asText=TRUE)
xpathSApply(parsedHmtl, "//title", xmlValue)
google = handle("http://google.com")
pg1 = GET(handle=google, path="/")
pg2 = GET(handle=google, path="search")
myapp = oauth_app("twitter", key = "mfPgl2zeDBVNUYFWbwpoFv4T9", secret = "mdpfKV11FAnZPhwJz2z0QQXqThTGd7zgBUZzQ4skZRgDAEjnto")
sig = sign_oauth1.0(myapp, token = "57149153-Jq3ShuGfzfBlGEILfy7oXHgginj29yCvLtgzVaoSu", token_secret = "GrPZGIp8e5Jya08fdudovsnOdqsnzqdY0G7zzINg4pra0
")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
library(jsonlite)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1, 1:4]
json2
myapp = oauth_app("twitter", key = "ft2cfdIxprxsDS4Y4dDME6ZSu
", secret = "5uy5i9JTlbKTXe67XlR0VyrjFHi0IwqZREx0LCnW0rKgJP7p7C
")
sig = sign_oauth1.0(myapp, token = "57149153-XwPGLmx6sDKvqwDQ0mgjCAMencW1wtZYYFEjiXyuy
", token_secret = "vLDyTI2dNdqMgD2rg85POwmnNlQKU4AVxVEnH22pFTKnh
")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
json1 = content(homeTL)
json1
json2 = jsonlite::fromJSON(toJSON(json1))
json2
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
homeTL
sig = sign_oauth1.0(myapp, token = "57149153-XwPGLmx6sDKvqwDQ0mgjCAMencW1wtZYYFEjiXyuy", token_secret = "vLDyTI2dNdqMgD2rg85POwmnNlQKU4AVxVEnH22pFTKnh")
myapp = oauth_app("twitter", key = "ft2cfdIxprxsDS4Y4dDME6ZSu", secret = "5uy5i9JTlbKTXe67XlR0VyrjFHi0IwqZREx0LCnW0rKgJP7p7C")
sig = sign_oauth1.0(myapp, token = "57149153-XwPGLmx6sDKvqwDQ0mgjCAMencW1wtZYYFEjiXyuy", token_secret = "vLDyTI2dNdqMgD2rg85POwmnNlQKU4AVxVEnH22pFTKnh")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
homeTL
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1, 1:4]
library(httpuv)
install.packages("httpuv")
library(httpuv)
install.packages("yaml")
install.packages("httpuv")
remove.packages("httpuv")
install.packages("httpuv")
library(httpuv)
remove.packages("httpv")
remove.packages("httpuv")
install.packages("httpuv")
library(httpuv)
install.packages("httpuv")
remove.packages("httpuv")
library(httpuv)
install.packages("httpuv")
library(httpuv)
install.packages("rccp")
.libPaths()
setwd("getting_and_cleaning_data/week4/getting_cleaning_data_project/")
source('C:/Users/rrj22/OneDrive/Desktop/data_science/technical_track/r_working_directory/getting_and_cleaning_data/week4/getting_cleaning_data_project/run_analysis.R')
source('C:/Users/rrj22/OneDrive/Desktop/data_science/technical_track/r_working_directory/getting_and_cleaning_data/week4/getting_cleaning_data_project/run_analysis.R')
names
newNames <- names(meanStdData)
newNames
newNames <- sub("^t", "time", newNames)
newNames
newNames <- sub("^f", "frequency", newNames)
newNames
newNames <- sub("Acc", "Accelerometer", newNames)
names(meanStdData)
newNames
newNames <- sub("Gyro", "Gyroscope", newNames)
newNames
newNames <- sub("-mean()-", "Mean", newNames)
newNames
newNames <- sub("-mean()-", "Mean", newNames)
newNames
newNames <- sub("-mean", "Mean", newNames)
newNames
newNames <- sub("-std", "StdDev", newNames)
newNames
newNames <- sub("()-", "", newNames)
newNames
newNames <- sub("()", "", newNames)
newNames
newNames <- sub("Mag", "Magnitude", newNames)
newNames
newNames <- sub("Freq", "Frequency", newNames)
newNames
newNames <- names(meanStdData)
newNames <- sub("^t", "time", newNames)
newNames <- sub("^f", "frequency", newNames)
newNames <- sub("Acc", "Accelerometer", newNames)
newNames <- sub("Gyro", "Gyroscope", newNames)
newNames <- sub("Freq", "Frequency", newNames)
newNames <- sub("-mean", "Mean", newNames)
newNames <- sub("-std", "StdDev", newNames)
newNames <- sub("-", "", newNames)
newNames
source('C:/Users/rrj22/OneDrive/Desktop/data_science/technical_track/r_working_directory/getting_and_cleaning_data/week4/getting_cleaning_data_project/run_analysis.R')
source('C:/Users/rrj22/OneDrive/Desktop/data_science/technical_track/r_working_directory/getting_and_cleaning_data/week4/getting_cleaning_data_project/run_analysis.R')
source('C:/Users/rrj22/OneDrive/Desktop/data_science/technical_track/r_working_directory/getting_and_cleaning_data/week4/getting_cleaning_data_project/run_analysis.R')
install.packages(c("plyr", "dplyr", "reshape2"))
source('C:/Users/rrj22/OneDrive/Desktop/data_science/technical_track/r_working_directory/getting_and_cleaning_data/week4/getting_cleaning_data_project/run_analysis.R')
source('C:/Users/rrj22/OneDrive/Desktop/data_science/technical_track/r_working_directory/getting_and_cleaning_data/week4/getting_cleaning_data_project/run_analysis.R')
source('C:/Users/rrj22/OneDrive/Desktop/data_science/technical_track/r_working_directory/getting_and_cleaning_data/week4/getting_cleaning_data_project/run_analysis.R')
melt
